

from selenium import webdriver
from bs4 import BeautifulSoup
import time
import re
import csv

driver = webdriver.Chrome()

all_data = []

page = 1
while True:
    url = f"https://stylenewstar.com/collections/bag2?page={page}"
    driver.get(url)
    time.sleep(3)

    soup = BeautifulSoup(driver.page_source, "lxml")

    product_name_div = soup.find_all('div', class_='product-name-title')
    if not product_name_div:
        print(f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {page} –ø—É—Å—Ç–∞. –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è.")
        break

    euro_prices = [
        span.get_text(strip=True)
        for span in soup.find_all("span", class_="product-new-price")
        if "‚Ç¨" in span.get_text()
    ]

    condition_divs = soup.find_all('div', class_='product-details-condition')
    conditions = [
        div.get_text(strip=True).replace("Condition:", "").strip()
        for div in condition_divs
    ]

    # --- –§–æ—Ç–æ ---
    image_urls = []

    # –Ω–∞—Ö–æ–¥–∏–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ç–æ–≤–∞—Ä—ã
    product_links = soup.select("a[href^='/products/']")

# –∏—Å–ø–æ–ª—å–∑—É–µ–º set, —á—Ç–æ–±—ã —É–±—Ä–∞—Ç—å –¥—É–±–ª–∏ –∏ —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤–∞—è –∫–∞—Ä—Ç–∏–Ω–∫–∞
    seen_products = set()
    for link in product_links:
        href = link.get("href")
        if href in seen_products:
            continue  # –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–æ–≤—Ç–æ—Ä–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ç–æ—Ç –∂–µ —Ç–æ–≤–∞—Ä
        seen_products.add(href)

        img_tag = link.find("img")
        if img_tag:
            src = img_tag.get("src")
            if src.startswith("//"):
                src = "https:" + src
            image_urls.append(
                src)
        else:
            image_urls.append("")

    # --- –ù–∞–∑–≤–∞–Ω–∏—è ---
    product_name = [div.get_text(strip=True) for div in product_name_div]

    english_set = set("ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz")
    allowed_chars = english_set | set(" &-")

    cleaned_names = []
    for name in product_name:
        cleaned = ''.join(char for char in name if char in allowed_chars)
        cleaned = ' '.join(cleaned.split())
        cleaned = re.sub(r'\s*[A-Z0-9-]{1,4}$', '', cleaned)
        cleaned = re.sub(r'\s*[A-Z]{1,3}\s*[A-Z]{1,3}$', '', cleaned)
        cleaned = cleaned.strip(" -")
        cleaned_names.append(cleaned)

    # --- –ë–µ—Ä—ë–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É ---
    min_len = min(len(cleaned_names), len(euro_prices), len(conditions), len(image_urls))

    # –î–æ–±–∞–≤–ª—è–µ–º –≤ –æ–±—â–∏–π —Å–ø–∏—Å–æ–∫
    for i in range(min_len):
        all_data.append([cleaned_names[i], euro_prices[i], conditions[i], image_urls[i]])

    print(f"‚úÖ –°–æ–±—Ä–∞–ª–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—É {page}")
    page += 1

driver.quit()

# --- –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å—ë –≤ CSV ---
with open("bags.csv", "w", newline="", encoding="utf-8-sig") as f:
    writer = csv.writer(f, delimiter=";")
    writer.writerow(["–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ", "–¶–µ–Ω–∞", "–ö–æ–Ω–¥–∏—Ü–∏—è", "–§–æ—Ç–æ"])
    writer.writerows(all_data)

print("üéâ –î–∞–Ω–Ω—ã–µ —Å–æ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ bags.csv")



# from selenium import webdriver
# from bs4 import BeautifulSoup
# import time
# import re
# import csv

# driver = webdriver.Chrome()
# driver.get("https://stylenewstar.com/collections/bag2")
# time.sleep(3)  # –∂–¥—ë–º –∑–∞–≥—Ä—É–∑–∫—É —Å—Ç—Ä–∞–Ω–∏—Ü—ã

# soup = BeautifulSoup(driver.page_source, "lxml")

# # --- –ù–∞–∑–≤–∞–Ω–∏—è ---
# product_name_div = soup.find_all('div', class_='product-name-title')
# product_name = [div.get_text(strip=True) for div in product_name_div]

# # --- –¶–µ–Ω—ã ---
# euro_prices = [
#     span.get_text(strip=True)
#     for span in soup.find_all("span", class_="product-new-price")
#     if "‚Ç¨" in span.get_text()
# ]

# # --- –ö–æ–Ω–¥–∏—Ü–∏—è ---
# condition_divs = soup.find_all('div', class_='product-details-condition')
# conditions = [
#     div.get_text(strip=True).replace("Condition:", "").strip()
#     for div in condition_divs
# ]

# # --- –§–æ—Ç–æ: –±–µ—Ä—ë–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—É—é –∫–∞—Ä—Ç–∏–Ω–∫—É –∫–∞–∂–¥–æ–≥–æ —Ç–æ–≤–∞—Ä–∞ ---
# # --- –§–æ—Ç–æ: —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤–∞—è –∫–∞—Ä—Ç–∏–Ω–∫–∞ –Ω–∞ —Ç–æ–≤–∞—Ä ---
# image_urls = []

# # –Ω–∞—Ö–æ–¥–∏–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ç–æ–≤–∞—Ä—ã
# product_links = soup.select("a[href^='/products/']")

# # –∏—Å–ø–æ–ª—å–∑—É–µ–º set, —á—Ç–æ–±—ã —É–±—Ä–∞—Ç—å –¥—É–±–ª–∏ –∏ —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤–∞—è –∫–∞—Ä—Ç–∏–Ω–∫–∞
# seen_products = set()
# for link in product_links:
#     href = link.get("href")
#     if href in seen_products:
#         continue  # –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–æ–≤—Ç–æ—Ä–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ç–æ—Ç –∂–µ —Ç–æ–≤–∞—Ä
#     seen_products.add(href)

#     img_tag = link.find("img")
#     if img_tag:
#         src = img_tag.get("src")
#         if src.startswith("//"):
#             src = "https:" + src
#         image_urls.append(f'=IMAGE("{src}")')
#     else:
#         image_urls.append("")



# # --- –û—á–∏—â–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è ---
# english_set = set("ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz")
# allowed_chars = english_set | set(" &-")

# cleaned_names = []
# for name in product_name:
#     cleaned = ''.join(char for char in name if char in allowed_chars)
#     cleaned = ' '.join(cleaned.split())
#     cleaned = re.sub(r'\s*[A-Z0-9-]{1,4}$', '', cleaned)
#     cleaned = re.sub(r'\s*[A-Z]{1,3}\s*[A-Z]{1,3}$', '', cleaned)
#     cleaned = cleaned.strip(" -")
#     cleaned_names.append(cleaned)

# # --- –ë–µ—Ä—ë–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É ---
# min_len = min(len(cleaned_names), len(euro_prices), len(conditions), len(image_urls))

# # --- –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ ---
# all_data = []
# for i in range(min_len):
#     all_data.append([cleaned_names[i], euro_prices[i], conditions[i], image_urls[i]])

# driver.quit()

# # --- –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ CSV ---
# with open("bags.csv", "w", newline="", encoding="utf-8-sig") as f:
#     writer = csv.writer(f, delimiter=";")
#     writer.writerow(["–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ", "–¶–µ–Ω–∞", "–ö–æ–Ω–¥–∏—Ü–∏—è", "–§–æ—Ç–æ"])
#     writer.writerows(all_data)

# print("‚úÖ –î–∞–Ω–Ω—ã–µ —Å –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ bags.csv")
